#+title: Parallel Computing with MATLAB on TRUBA HPC
#+author: sbilmis
#+language: en
#+options: toc:t num:t ^:nil
#+startup: content



* Introduction

This document describes the configuration and usage of MATLAB on the TRUBA (HPC) infrastructure, including license usage,
interactive execution, and batch job submission via Slurm.

This guide is intended for academic users of TRUBA.

** Why use TRUBA for MATLAB?
- **Speed:** Run heavy simulations faster using high-end CPUs.
- **Memory:** Handle much larger datasets than a standard laptop can manage.
- **Parallel Computing:** Run multiple MATLAB "workers" simultaneously to solve problems in a fraction of the time.



** Tested Environment

*Note:* This documentation was validated under the following environment. Changes in MATLAB versions or Slurm settings may require minor adjustments.

 | Component           | Version / Value              |
 |--------------------|------------------------------|
 | Cluster            | TRUBA (arf and cuda)         |
 | Partitions Tested  | debug                        |
 | MATLAB             | R2025b                       |
 | License Model      | Network Academic (MathWorks) |
 | Slurm              | 23.02.5-1                    |
 | OS                 | Rocky Linux 9                |
 | Test Date          | 2026-02-18                   |


 This guide reflects the configuration of TRUBA as of the test date above.
 Changes in MATLAB versions, Slurm configuration, or license settings may
 require adjustments.


* Prerequisites & Licensing

Ensure you have these three requirements ready before proceeding:
- **Active TRUBA Account:** A valid username and password.
- **VPN Connection:** **OpenVPN** must be active to access the cluster.
- **Academic Purpose:** MATLAB on TRUBA is strictly for academic research. Commercial/corporate use is not permitted.
  

** Licensing and Policy
TRUBA provides MATLAB under a sponsored *Network Academic* license. 

#+BEGIN_IMPORTANT
**Important:** Only *academic* MATLAB usage is permitted on TRUBA compute resources. 
#+END_IMPORTANT


** What this means in practice
How the license is handled depends on how you choose to work:

- **Using Open OnDemand or Batch Mode:** If you run MATLAB directly on the TRUBA cluster (via your web browser or a batch script), the license is provided and managed automatically on the cluster side. You do not need to provide your own.
- **Using MATLAB on your Personal Computer:** If you write code in your local MATLAB and submit jobs to TRUBA using the cluster plugin, your local installation **must** be an *academic* license. If you use a commercial or trial license on your laptop, the remote execution will be blocked.

#+BEGIN_TIP
*Not sure if your license is Academic?*
Open MATLAB on your computer and type =license= in the Command Window. If you are still unsure, contact your universityâ€™s software coordinator or IT department.
#+END_TIP



* Ways to Run MATLAB on TRUBA

There are three supported methods:

1. Open OnDemand (interactive GUI) â€“ Recommended for beginners.
2. Slurm Batch Script â€“ Recommended for production HPC jobs.
3. Local MATLAB Cluster Profile (batch) â€“ Advanced workflow.



* Configuring MATLAB to Run on the HPC Cluster (Web Browser via Open OnDemand)
This method allows you to use a full graphical MATLAB interface directly in your web browser (Open OnDemand). It is the best choice if you want to work interactively.

#+BEGIN_IMPORTANT
**Rule:** You must launch MATLAB through Open OnDemand. Running MATLAB directly on "login nodes" is strictly prohibited and will be terminated.
#+END_IMPORTANT

** Accessing the ARF Cluster
- Connect to **TRUBA OpenVPN**.
- Open your browser and go to: [[https://172.16.6.20]]
- Log in with your TRUBA username and password.
- From the dashboard, click on **Interactive Apps** and select **Desktop**.

For detailed connection instructions on OpenOnDemand usage on TRUBA, refer to:
https://docs.truba.gov.tr

#+CAPTION: Open OnDemand dashboard for ARF cluster
[[file:images/arf_ood_dashboard.png]]


** Starting MATLAB in the Desktop Session
Once your virtual desktop appears in the browser:

- **Open a Terminal:** Right-click anywhere on the desktop and select *Open Terminal Here*.
   

- **Load MATLAB:** Type the following commands to prepare the environment (we use the latest version by default):

#+begin_src bash
module purge
module available | grep -i matlab # to see the available modules installed (we generally keep the latest 4 version of the MATLAB on the cluster
module load apps/matlab/r2025b
#+end_src

- **Launch MATLAB:** Run the command below. We add =-nosplash= to make it open faster and remove the startup splash screen.

#+begin_src bash
matlab -nosplash
#+end_src



** Introducing MATLAB to the Cluster
Even though you are on the cluster, MATLAB needs to "discover" its surroundings the first time you run it.

1. In the MATLAB **Home** tab, click **Parallel** -> **Discover Clustersâ€¦**
2. MATLAB will detect the specific compute node you were assigned (e.g., =orfoz34=).
3. Follow the prompts to finish.


#+CAPTION: Discover Clusters window showing allocated compute node
[[file:images/matlab_discover_clusters.png]]


#+BEGIN_NOTE
**What does this step do?** This simply tells MATLAB that it is running on a TRUBA compute node. It does *not* set up long-term job submission (we will do that in the next section).
#+END_NOTE



** Important: The "Validate" Button
If you open the *Cluster Profile Manager* (Home â†’ Parallel â†’ Cluster Profile Manager), you will see a **Validate** button. 

#+BEGIN_IMPORTANT
**Do not use the "Validate" button here.** On TRUBA, job execution is handled by the Slurm manager, not MATLAB's internal validator. The validation test will likely fail, but this **does not** mean your MATLAB is broken. As long as you can run commands, your setup is correct.
#+END_IMPORTANT



** Switching Between ARF and CUDA Clusters
TRUBA has two main sections: **ARF** (Standard) and **CUDA** (GPU-focused).

- **ARF Access:** [[https://172.16.6.20]]
- **CUDA Access:** [[https://172.16.6.16]] (Requires special authorization)

  Access to the CUDA cluster requires authorization. See the official documentation:

https://docs.truba.gov.tr/1-kaynaklar/arf_acc/arf_acc_baglanti.html#arf-acc-baglanti

  
Your files, settings, and MATLAB preferences are shared between both clusters. 

- **Settings Folder:** =~/.matlab/= (This stores your preferences).
- **Home Directory:** =/arf/home/$USER= (This is where your files live).



* Configuring MATLAB to Submit Jobs from Local MATLAB to TRUBA

This section describes how to configure MATLAB installed on your local
computer to submit jobs remotely to the TRUBA cluster.

This configuration:

- Is required only if MATLAB is installed on your personal machine.
- Must be performed once per Slurm cluster (e.g., ARF or CUDA). 
- Must be repeated for each MATLAB version installed locally.

** Verify Your User Path

MATLAB needs a specific folder to store the TRUBA connection scripts.
- Open MATLAB on your computer.
- Type =userpath= in the Command Window.
- If it returns a folder, that is where we will put the plugin. 
- If it is empty, type =userpath('reset')= to fix it.


#+caption: MATLAB userpath after reset
[[file:images/local_userpath_output.png]]

If necessary, manually create a MATLAB directory under your
Documents folder and set it as the user path:

#+begin_src matlab
mkdir(fullfile(getenv('HOME'),'Documents','MATLAB'))
userpath(fullfile(getenv('HOME'),'Documents','MATLAB'))
savepath
#+end_src


** Install the TRUBA Cluster Plugin

- Download the **TRUBA MATLAB Plugin (ZIP)??????? Add the folder link**.
- Extract the files directly into the folder you found in the step above.
- In the MATLAB Command Window, run:
  (This will create a TRUBA cluster profile configured for Slurm submission.)
  #+begin_src matlab
  configCluster
  #+end_src
- Follow the prompts: Select the cluster (ARF or CUDA) and enter your **TRUBA username**. 

#+BEGIN_NOTE
You won't be asked for a password yet. You will enter your password only when you actually "send" a job to the cluster.
#+END_NOTE

#+caption:  Running configCluster and selecting TRUBA ARF
[[file:images/configCluster_execution.png]]
   
After successful configuration, jobs submitted from MATLAB
will run on the TRUBA cluster instead of the local machine.

You can also see  the cluster profile from GUI Home -> Parallel -> Create and Manage Clusters  
#+caption:  TRUBA ARF cluster profile visible in Cluster Profile Manager
[[file:images/cluster_profile_manager.png]]


** Switching Between Your Computer and the Cluster
 While TRUBA handles the massive calculations, you will still need to run small, immediate tasks on your own computer's hardware. This is managed through different "Profiles."

*** Setting Up Your Local Profile
 Before you start, ensure you have a "local" profile to handle work on your own laptop.
 - Go to the **Home** tab in MATLAB.
 - Click **Parallel** > **Create and Manage Clusters**.
 - Check the list on the left. If you do not see a profile named **Processes** or **local**, click **Add Cluster Profile** > **Processes**.
 - This ensures you can always switch back to "Laptop mode" when you aren't using the cluster.

#+caption:  TRUBA ARF cluster profile visible in Cluster Profile Manager
[[file:images/cluster_profile_manager.png]]

*** How to Switch Profiles in Your Code
 You can tell MATLAB where to run your work by using a simple command. This is helpful if you want to test a small part of your code locally before sending the whole thing to TRUBA.

 - **To work on TRUBA (Default):**
   #+begin_src matlab
  c = parcluster; % This loads your default TRUBA profile
   #+end_src

 - **To work on your own Laptop:**
   #+begin_src matlab
  % This tells MATLAB to use your computer's own processors
  c = parcluster('Processes'); 
   #+end_src

 #+BEGIN_TIP
 *Rookie Tip:* Think of the "Processes" profile as your personal workspace and the "TRUBA" profile as the heavy-duty factory. Only send work to the factory when it is too big for your workspace!
 #+END_TIP





 
* Configuring Jobs (Scheduler Parameters (Slurm Options))

Before submitting jobs from MATLAB to TRUBA, users are strongly advised to have
basic familiarity with Slurm concepts such as partitions, wall time, CPUs, and
memory and available partitions.

- Slurm job script fundamentals:
  https://docs.truba.gov.tr/2-temel_bilgiler/slurm-betik-ozellik.html

- Overview of TRUBA computing clusters / partitions:
  https://docs.truba.gov.tr/2-temel_bilgiler/hesaplama_kumeleri.html



** Understanding Slurm: The Cluster "Booking System"
Before you send work to TRUBA, you need to understand three basic concepts. Think of it like booking a hotel room:
- **Partition:** Which "wing" of the building you want to stay in (e.g., standard, luxury, or specialized GPU rooms).
- **WallTime:** How long you intend to stay. If you stay past checkout, the system will automatically end your session.
- **Resources:** How many beds (CPUs) and how much space (Memory) you need.


*** Listing Available Partitions from MATLAB

NOTE: Documentation pages may not always reflect the most up-to-date partition
configuration. You can query the available partitions directly from MATLAB using
the TRUBA cluster profile.
    
 #+begin_src matlab
c = parcluster;
clusterPartitionNames(c)
 #+end_src

 Example output:

 #+begin_example
{'akya-cuda' }
{'barbun' }
{'barbun-cuda'}
{'debug' }
{'hamsi' }
{'orfoz' }
{'smp' }
 #+end_example


** Slurm Options Exposed by the TRUBA MATLAB Profile

To view the current slurm site configuration used by the profile:

#+begin_src matlab
c = parcluster;
c.AdditionalProperties
#+end_src


** What You Typically Need to Specify

When submitting jobs to TRUBA, you should generally define at least:

1. *Partition* (=c.AdditionalProperties.Partition=)
   - For initial testing, we recommend using =debug=.

2. *Requested compute size* (MATLAB workers)
   - When submitting jobs from MATLAB, the size of the job is typically controlled
     by the *pool size* (see the minimal test job below).
   - Some partitions enforce policies on job size. For example, on =orfoz= you may
     be required to request 56Ã—n (where n is integer) cores.

3. *Wall time* (=c.AdditionalProperties.WallTime=)
   - This is required.
   - Keep it short for tests (e.g., 15â€“2 minutes).
   - Maximum wall time depends on partition/site policy.


** Your First "Hello World Job"  (Recommended)

This minimal test submits a simple job that returns the working directory (=pwd=).
It is designed to be small, predictable, and suitable for first-time validation.

We explicitly set:

- =Partition= to =debug= (small jobs are accepted)
- =NumNodes= to =1= (avoid accidental multi-node allocation for a tiny test)
- =WallTime= to a short value

#+begin_src matlab
c = parcluster;

% Recommended baseline for a first test
c.AdditionalProperties.Partition = 'debug';
c.AdditionalProperties.NumNodes  = 1;
c.AdditionalProperties.WallTime  = '0-00:10';

% Submit a pool job (Pool>=2) and disable client-path propagation
j = batch(c, @pwd, 1, {}, ...
    'Pool', 2, ...
    'CurrentFolder', '.', ...
    'AutoAddClientPath', false);
#+end_src

When this job is submitted, MATLAB generates a Slurm submission script.
You can see the actual Slurm flags in the submission output.


*** Understanding the =batch= Command

 The following command submits a job to the TRUBA cluster:

 #+begin_src matlab
j = batch(c, @pwd, 1, {}, ...
    'Pool', 2, ...
    'CurrentFolder', '.', ...
    'AutoAddClientPath', false);
 #+end_src

 Explanation of the arguments:

 - =c=  
   The TRUBA cluster profile created earlier.

 - =@pwd=  
   The function to execute on the cluster.  
   In this minimal test, =pwd= simply returns the working directory.

 - =1=  
   Number of output arguments expected from the function.

 - ={}=  
   Cell array of input arguments to the function (empty in this example).

 - ='Pool', 2=  
   Starts 2 MATLAB workers on the cluster.  
   On TRUBA ARF, independent single-core jobs are not supported, so
   a pool size of at least 2 is required for this test.

 - ='CurrentFolder', '.'=  
   Specifies the working directory from which the job is submitted.

 - ='AutoAddClientPath', false=  
   Prevents MATLAB from attempting to add local (e.g., macOS/Windows)
   directories to the cluster workers' search path.
   This avoids unnecessary warnings.

   For convenience, the same command can be written in a single line:

 Internally, the pool size determines how many parallel worker processes
 are launched on the cluster.


 #+begin_src matlab
j = batch(c,@pwd,1,{},'Pool',2,'CurrentFolder','.', 'AutoAddClientPath',false);
 #+end_src

- When you run this command you will be asked the password you login to cluster:

Typical submission arguments may look like:

#+begin_example
--ntasks=3
--cpus-per-task=1
-N 1
-p debug
-t 0-00:10
#+end_example



 NOTE:
 - On TRUBA ARF, submitting an independent single-core job without a pool may fail
   with: "Single core jobs are not supported". For this reason, we use =Pool= in the
   minimal test.


** The "Pool + 1" Rule: How Cores are Counted
 This is the most common reason jobs are rejected. On TRUBA, if you ask for a parallel **Pool**, MATLAB uses **one extra CPU** to manage the workers.

 #+BEGIN_IMPORTANT
 **Total CPUs Used = Pool Size + 1**
 If you set your Pool to **2**, Slurm sees a request for **3** CPUs.
 #+END_IMPORTANT

*** Why does this matter?
 Some TRUBA partitions require you to request CPUs in specific "blocks" (multiples). For example, on the **orfoz** partition, you must request 56xn (where n is integer)  cores.
 - To use exactly **56 cores**, you must set your Pool to **55**.
 - (55 Workers + 1 Manager = 56 total).


** Partition Policies on TRUBA

TRUBA enforces additional partition-specific job size policies at submission time.
These policies are checked automatically by the scheduler. If a job does not comply,
Slurm rejects it with an explanatory message.

The key idea is that policies are applied *per node*:

- Per-node CPU request = (Total requested CPUs) / (NumNodes)

*** Summary of node sizes (informative)

From =scontrol show partition= output, the typical CPU core count per node is:

- =orfoz=: 112 CPU cores per node
- =hamsi=: 56 CPU cores per node
- =barbun=: 40 CPU cores per node
- =akya-cuda=: 40 CPU cores per node
- =barbun-cuda=: 40 CPU cores per node

*** CPU partitions: per-node CPU count rules

- =orfoz=
  - Requests must be *56Ã—n* CPU cores per node (commonly 56 or 112).
  - If violated, Slurm may reject the job with a message such as:
    "Orfoz kuyruguna gonderilen islerde node basina 56/112 cekirdek talep ediniz."

- =hamsi=
  - Requests must be *56Ã—n* CPU cores per node.

- =barbun=
  - Requests must be *40Ã—n* CPU cores per node.

Practical recommendation:
- Use =debug= for small test jobs.
- Use the target partition for production jobs and request per-node CPU cores
  in the allowed multiples.

*** GPU partitions: GPU request and CPUâ†”GPU ratio rules

- =akya-cuda=
  - A GPU request is mandatory.
  - CPU cores must be requested in multiples of 10.
  - Policy requires 1 GPU per 10 CPU cores per node.

- =barbun-cuda=
  - A GPU request is mandatory.
  - CPU cores must be requested in multiples of 20.
  - Policy requires 1 GPU per 20 CPU cores per node.

NOTE:
The exact GPU request syntax depends on the submission method.
When submitting through the TRUBA MATLAB plugin, GPU requests are set via
the cluster profile options (see the GPU example section later in this document).


*** Working directory policy

TRUBA requires jobs to run from =/arf/scratch= (recommended).
Jobs submitted from other directories may be rejected by site policy.

For MATLAB submissions, we recommend using:

#+begin_src matlab
'CurrentFolder','.'
#+end_src

when MATLAB is started in a directory under =/arf/scratch=.



 

** CPU and Memory Relationship (Conceptual Model)

On TRUBA, most partitions follow a *memory-per-CPU* allocation model.

This means:

- Memory allocation scales with the number of requested CPU cores.
- Requesting more cores results in proportionally more memory.

For example (conceptual):

If a node has:

- 112 CPU cores
- 256 GB RAM

Then approximate memory per core is:

=256 / 112 â‰ˆ 2.3 GB per core=

If your job requires 100 GB RAM:

=100 / 2.3 â‰ˆ 44 cores=

Even if your application is not CPU-intensive,
you may need to request additional cores to obtain sufficient memory.

*** Important: Responsible Resource Usage

Although increasing core count increases available memory:

- Do not request excessive cores unnecessarily.
- Choose a partition with higher memory-per-core when appropriate.

For example (based on current TRUBA configuration):

- =orfoz= â†’ ~2000 MB per CPU
- =hamsi= â†’ ~3400 MB per CPU
- =barbun= â†’ ~8500 MB per CPU

For memory-heavy workloads, selecting a partition with higher
memory per CPU may reduce unnecessary CPU allocation.





** Resource Selection Strategy

Before submitting your job, identify the workload type:

*** CPU-bound Jobs

- Heavy numerical computation
- Parallel loops
- Linear algebra

â†’ Increase *Pool* size appropriately.
â†’ Match partition core constraints (e.g., 56-core blocks on =orfoz=).

*** Memory-bound Jobs

- Large matrices
- Large data loading
- In-memory preprocessing

â†’ Estimate required memory.
â†’ Compute approximate cores needed via memory-per-CPU model.
â†’ Prefer partitions with higher DefMemPerCPU (e.g., =barbun=).

*** GPU-bound Jobs

- Deep learning
- CUDA-based computation

â†’ Use GPU partitions (=akya-cuda=, =barbun-cuda=).
â†’ Ensure GPU-to-CPU ratio follows partition policy.



** Monitoring the Job

After submitting the job, you can monitor its status both from MATLAB
and from the TRUBA command line.

*** From MATLAB

Check the job state:

#+begin_src matlab
j.State
#+end_src

Possible states include:

- **queued:** You are in line.
- **running:** Calculations are happening.
- **finished:** Success!
- **failed:** Check your code for errors.

You can also inspect timing information:

#+begin_src matlab
j.SubmitTime
j.StartTime
j.FinishTime
#+end_src

 Instead of checking the state manually over and over, you can tell MATLAB to pause and wait until the job is done.

 #+begin_src matlab
% This command pauses your MATLAB until the job finishes
wait(j)
 #+end_src


*** From the TRUBA Login Node (Slurm)

If you know the Slurm job ID (visible in submission output),
you can check:

#+begin_src bash
squeue -u $USER
#+end_src

After completion:

#+begin_src bash
sacct -j <jobid> --format=JobID,State,Elapsed,AllocCPUS,NodeList
#+end_src



** Fetching and Inspecting Job Results

Once the job state becomes ='finished'=, you can retrieve outputs and inspect logs.

*** Fetch outputs

#+begin_src matlab
out = fetchOutputs(j);
out{1}
#+end_src

NOTE:
Outputs are returned as a cell array even if there is a single output.

*** If the job failed

If =j.State= is ='failed'=, inspect the task error message:

#+begin_src matlab
j.Tasks(1).Error
#+end_src


#+BEGIN_TIP
**Pro Tip:** If your job failed, type =diary(j)=. This will print everything the cluster tried to say to the command window, including hidden error messages.
#+END_TIP

#+begin_src matlab
diary(j)
#+end_src

*** Working with multiple test jobs

If you closed MATLAB and lost the variable ~j~ or if you submit multiple jobs and no longer have the variable =j=,
you can list jobs associated with the cluster profile:

#+begin_src matlab
c = parcluster;
c.Jobs
#+end_src

To select the most recent job:

#+begin_src matlab
j = c.Jobs(end);
#+end_src



Then fetch outputs as usual:

#+begin_src matlab
out = fetchOutputs(j);
out{1}
#+end_src

*** Optional cleanup

To remove a completed job object and associated local metadata:

#+begin_src matlab
delete(j)
#+end_src
*** Deleting All Jobs from the Profile

To remove all stored job objects:

#+begin_src matlab
c = parcluster;
delete(c.Jobs)
#+end_src

This clears local job metadata stored in the cluster profile's
job storage directory.
































--------------------------------



** Modifying or Unsetting Settings

You can change a value by assigning a new one, for example:

#+begin_src matlab
c.AdditionalProperties.WallTime = '0-00:30';
#+end_src

To unset values (return to plugin defaults), use empty/zero/false values, e.g.:

#+begin_src matlab
c.AdditionalProperties.Partition = '';
c.AdditionalProperties.NumNodes  = 0;
c.AdditionalProperties.MemPerCPU = '';
c.AdditionalProperties.RequireExclusiveNode = false;
#+end_src




** Saving Settings 

If you want MATLAB to reuse your current configuration in future sessions,
save the cluster profile:

#+begin_src matlab
c.saveProfile
#+end_src

If you do not save the profile, changes are typically not persistent between
MATLAB sessions. If you have not saved the profile and want to revert all changes,
simply restart MATLAB.






---------------------------



* Advanced: Interactive Parallel Jobs from MATLAB on the Desktop

This section describes how to launch a parallel pool on TRUBA directly
from MATLAB running on your personal computer.

This workflow is recommended only for advanced users who need
interactive development while using cluster resources.

**Important:** This mode requires proper network configuration
between your computer and TRUBA worker nodes.

---

**Prerequisites**

To run an interactive pool job launched from your desktop onto TRUBA,
the following conditions must be satisfied:

1. You must be connected to the institutional VPN.
2. TCP port 27370 must be open for inbound traffic on your machine.
3. MATLAB must advertise your VPN private IP address to worker nodes.

To configure the hostname inside MATLAB:

#+begin_src matlab
sethostname
#+end_src

Expected output:

#+begin_src matlab
Found private IP address.  Setting hostname: 10.x.x.x
#+end_src

This command must be executed every time MATLAB is restarted.
Alternatively, add it to your `startup.m` file.

If your firewall blocks communication, you may see an error such as:

"Check whether a firewall is blocking communication between the worker machines and the MATLAB client machine."

---

**Starting a Parallel Pool on TRUBA**

#+begin_src matlab
% Get cluster profile
c = parcluster;

% Start 28 workers (example)
pool = c.parpool(28);
#+end_src

Explanation:

- `parcluster` connects MATLAB to the TRUBA cluster profile.
- `parpool(28)` submits a Slurm job that allocates 28 workers.
- Workers may span multiple nodes depending on scheduler decisions.

The pool remains active until explicitly deleted.

---

** Interactive Parallel Work
Instead of running a local pool on your own computer, your code can now run across multiple nodes on the TRUBA cluster simultaneously.

#+begin_src matlab
% Single-line test: Measure time to generate 1,000 numbers and display the sum
tic; parfor idx = 1:1000, a(idx) = rand; end; result = sum(a), toc
#+end_src



** Closing the Pool
When you are finished with your parallel tasks, it is a good practice to shut down the pool to free up cluster resources:

#+begin_src matlab
delete(gcp('nocreate'))
#+end_src

** TRUBA Resource Constraints (Important)

Desktop-based interactive pools do not bypass TRUBA scheduling rules.

Even when launching pools from MATLAB on your desktop,
all TRUBA partition constraints still apply.

The pool creation command internally submits a Slurm job.
If resource limits are violated, the job submission will fail.

Common error messages include:

#+begin_src text
sbatch: error: Orfoz kuyruguna gonderilen isleride node basina 56/112 cekirdek talep ediniz.
sbatch: error: Batch job submission failed: Requested time limit is invalid (missing or exceeds some limit)
#+end_src

These errors indicate that your requested configuration does not satisfy partition limits. See the above section for setting these parameters.

Typical causes:

- Requesting more workers than allowed per node
- Exceeding maximum walltime
- Using incorrect partition
- Not specifying required time limit

Before adjusting `parpool(N)`, verify:

- The partition you are using
- Maximum cores allowed per node
- Maximum allowed walltime
- Whether multi-node execution is permitted

Always ensure that:

  requested workers â‰¤ allowed cores per node

If necessary, reduce the pool size or adjust it according to the cluster rules:

#+begin_src matlab
pool = c.parpool(56);
#+end_src

or adjust cluster profile settings as documented earlier.


**Simple Numerical Example**

#+begin_src matlab
tic;
R = zeros(1,280);
parfor i = 1:280
    R(i) = sum(svd(rand(400)));
end
toc;
mean(R)
#+end_src

Explanation:

- 280 iterations ensure sufficient work per worker.
- Each iteration computes singular values of a random 400x400 matrix.
- `mean(R)` provides numerical output.
- `tic/toc` measures elapsed wall time.

To compare with serial execution:

#+begin_src matlab
tic;
for i = 1:280
    sum(svd(rand(400)));
end
toc
#+end_src

Parallel execution should show noticeable speedup for sufficiently large workloads.

---

**Understanding Overhead**

When you call `parpool`, MATLAB:

- Submits a Slurm job
- Allocates nodes
- Starts worker MATLAB processes
- Performs license checks
- Establishes TCP connections
- Synchronizes the environment

This startup phase can take from tens of seconds to several minutes.

If the actual computation is small (e.g., `parfor i=1:10, rand; end`),
startup overhead dominates and parallel execution may appear slow.

Parallel computing is beneficial only when:

  computation time  >>  startup and communication overhead

---

**Monitoring Jobs**

When a pool is active, MATLAB Job Monitor shows:

  Description: Interactive pool
  State: running

This indicates that workers are allocated and idle or ready.

It does NOT necessarily mean that a `parfor` loop is currently executing.

As long as the pool is open, cluster resources remain allocated.

** Opening Job Monitor

To view active pools and cluster jobs, open:

Parallel â†’ Monitor Jobs

#+CAPTION: Opening the Job Monitor from the MATLAB Parallel menu.
#+NAME: fig:open_job_monitor
[[file:images/open_job_monitor.png]]

---

** Example: Job Monitor Window

#+CAPTION: MATLAB Job Monitor showing running and finished jobs.
#+NAME: fig:job_monitor_window
[[file:images/job_monitor_window.png]]

In this window:

- *Description: Interactive pool* indicates that a parallel pool is active.
- *State: running* means that worker processes are allocated.
- This does **not** necessarily mean that a `parfor` loop is currently executing.

As long as the pool remains open, cluster resources stay allocated.


---

**Closing the Pool**

Always delete the pool after finishing:

#+begin_src matlab
delete(gcp)
#+end_src

Failing to close the pool wastes compute resources.

Idle pools may be terminated automatically after a timeout
(currently ~30 minutes of inactivity; subject to change).

---

**Best Practice Recommendation**

- Use this mode for interactive development only.
- For production runs, use batch submission (`sbatch`).
- If working inside an OnDemand desktop session, prefer a local pool
  (`parpool('local',N)`) instead of submitting a second cluster job.







* TRUBA MATLAB Helper Functions

TRUBA provides several helper functions that simplify interaction
with Slurm and cluster resources from within MATLAB.

These functions are optional but strongly recommended
for advanced usage and troubleshooting.

| Function              | Description                                    | Notes                                     |
|-----------------------+------------------------------------------------+-------------------------------------------|
| clusterFeatures       | Lists cluster features and node constraints    | Useful for understanding partition limits |
| clusterGpuCards       | Lists available GPU models on TRUBA            | Use before requesting GPU resources       |
| clusterPartitionNames | Lists available Slurm partitions/queues        | Verify before setting profile             |
| fixConnection         | Reestablishes cluster connection               | Applicable only for Desktop mode          |
| seff                  | Displays Slurm efficiency statistics for a job | Similar to Linux `seff` command           |
| willRun               | Explains why a job is queued                   | Helps diagnose scheduling issues          |

---

**Example Usage**

#+begin_src matlab
clusterPartitionNames
#+end_src

Lists available TRUBA partitions.

#+begin_src matlab
clusterFeatures
#+end_src

Displays node-level constraints such as maximum cores per node.

#+begin_src matlab
seff(326528)
#+end_src

Displays efficiency statistics for the given Slurm job ID.

#+begin_src matlab
willRun(326528)
#+end_src

Explains why a job is currently queued.

#+begin_src matlab
fixConnection
#+end_src

Useful after reconnecting to VPN if desktop pools fail.


* Troubleshooting and Debugging

When a job fails or behaves unexpectedly, MATLAB provides
tools to retrieve log files and scheduler information.

---

**Retrieving Debug Logs**

If a serial or batch job produces an error,
use the `getDebugLog` method to view the error log.

For independent jobs (multiple tasks):

#+begin_src matlab
c.getDebugLog(job.Tasks)
#+end_src

For pool jobs:

#+begin_src matlab
c.getDebugLog(job)
#+end_src

The debug log contains detailed error messages generated
by MATLAB workers on TRUBA.

---

**Retrieving Scheduler Job ID**

Cluster administrators may request the Slurm job ID
for troubleshooting purposes.

You can obtain the scheduler ID by calling:

#+begin_src matlab
job.getTaskSchedulerIDs()
#+end_src

Example output:

#+begin_src matlab
ans =
    25539
#+end_src

This number corresponds to the Slurm job ID
visible via `squeue` or `sacct` on TRUBA.


** Licensing Issues

MATLAB jobs running on TRUBA require valid licenses for:
- MATLAB
- Parallel Computing Toolbox (for pool jobs / parpool / batch with Pool)

Symptoms of licensing problems may include:
- Job fails immediately after submission
- Workers cannot start
- parpool hangs or reports license checkout failures

*** Check license availability (local MATLAB)

#+begin_src matlab
license('test','Distrib_Computing_Toolbox')
#+end_src

Return value:
- 1  -> toolbox license available
- 0  -> not available

#+begin_src matlab
license('inuse')
#+end_src

*** Check license status on TRUBA (if permitted)

#+begin_src bash
lmstat -a
#+end_src

If you cannot access license tools on TRUBA, include the Slurm job ID and job debug logs
when contacting TRUBA support (see: Retrieving Debug Logs / Retrieving Scheduler Job ID).





* A parfor Example (Four Execution Methods)

In this section, we demonstrate the same ~parfor~ example using four different
execution methods:

1. Submitting from a local MATLAB session configured with the TRUBA cluster profile
2. Running interactively on TRUBA with ~parpool~
3. Submitting via a Slurm batch script
4. Submitting via Open OnDemand

The test function used in all cases is shown below.

#+begin_src matlab
function t = parfor_demo()
% Simple parfor timing demo (runs on workers)
N = 55;

tic
parfor i = 1:N
    pause(1);
end
t = toc;

fprintf("parfor_demo: N=%d, elapsed=%.2f s\n", N, t);
end
#+end_src

We use the ~orfoz~ partition with 56 cores per node. Since MATLABâ€™s Slurm
integration uses one orchestration task in addition to the worker pool,
we set:

- ~Pool = 55~
- ~NumNodes = 1~

so that the total requested tasks become 56.

** Why Parallelization Matters

Each iteration takes approximately one second (~pause(1)~).  
With ~N = 55~, a serial loop would take approximately:

#+begin_example
55 iterations Ã— 1 second â‰ˆ 55 seconds
#+end_example

When using ~parfor~ with a pool of 55 workers, all iterations are dispatched
concurrently in a single execution wave. The theoretical minimum runtime is:

#+begin_example
â‰ˆ 1 second
#+end_example

In practice, the measured runtime is slightly higher (e.g., around
1.8â€“2.0 seconds) due to parallel overhead, including:

- Task scheduling
- Worker communication
- Synchronization barriers
- System-level latency

Even with this overhead, the wall-clock time decreases from nearly one
minute to just a few seconds. This confirms that the allocated CPU cores
are actively utilized instead of remaining idle.


** Submitting from a Local MATLAB Session (Cluster Profile)

*** File Location and Execution Model

 When MATLAB runs on a local machine using a TRUBA cluster profile,
 the ~.m~ file does not need to be manually copied to TRUBA.

 During job submission, MATLAB:

 1. Packages the required function files
 2. Transfers them to TRUBA
 3. Submits the job to Slurm
 4. Executes the job on compute nodes

 The file must be located in the current working directory or on the
 MATLAB path so that it can be staged correctly.

 Once submitted, the job runs entirely on TRUBA compute nodes, and the
 parallel workers execute on the allocated resources.

*** Cluster Configuration

 #+begin_src matlab
c = parcluster;
c.AdditionalProperties.Partition = 'orfoz';
c.AdditionalProperties.NumNodes  = 1;
c.AdditionalProperties.WallTime  = '0-00:30';
 #+end_src

*** Job Submission

 #+begin_src matlab
j = batch(c, @parfor_demo, 1, {}, ...
    'Pool', 55, ...
    'CurrentFolder'1, '.', ...
    'AutoAddClientPath', false);
 #+end_src

*** Job Submission (File Already Exists on TRUBA)

If the script already exists on TRUBA (for example at:

#+begin_example
/arf/scratch/sbilmis/parfor_demo.m
#+end_example

and you are submitting from a local MATLAB session configured
with the TRUBA cluster profile, you do NOT change your local
working directory to /arf/scratch.

Instead, you specify the remote execution directory using
~CurrentFolder~.

#+begin_src matlab
j = batch(c, @parfor_demo, 1, {}, ...
    'Pool', 55, ...
    'CurrentFolder', '/arf/scratch/sbilmis', ...
    'AutoAddClientPath', false);
#+end_src

Explanation:

- ~@parfor_demo~ refers to the function name (parfor_demo.m).
- The file must already exist in the specified remote directory.
- ~CurrentFolder~ tells TRUBA where to execute the job.
- ~AutoAddClientPath = false~ prevents MATLAB from trying to
  stage local files to the cluster.
- ~Pool = 55~ results in 56 Slurm tasks (55 workers + 1 client).

In this mode, no file transfer occurs. MATLAB simply instructs
Slurm to execute the script directly from the shared TRUBA filesystem.


 
*** Retrieve Output

 #+begin_src matlab
wait(j);
diary(j)          % shows printed text (including elapsed time)
t = fetchOutputs(j);
delete(j);
 #+end_src

 The following screenshot shows the Slurm submission arguments and the
 measured execution time:

 #+attr_html: :width 900px
 #+caption: Slurm submission arguments and parfor execution time on orfoz (55 workers).
 [[file:first.png]]

 This runtime confirms that 55 CPU cores were actively utilized instead of remaining idle.



*** Job Submission (File Already Exists on TRUBA)

 If the script already exists on TRUBA (for example at:

 #+begin_example
/arf/scratch/sbilmis/parfor_demo.m
 #+end_example

 and you are submitting from a local MATLAB session configured
 with the TRUBA cluster profile, you do NOT change your local
 working directory to /arf/scratch.

 Instead, you specify the remote execution directory using
 ~CurrentFolder~.

 #+begin_src matlab
j = batch(c, @parfor_demo_on_arf, 1, {}, ...
    'Pool', 55, ...
    'CurrentFolder', '/arf/scratch/sbilmis', ...
    'AutoAddClientPath', false);
 #+end_src

 Explanation:

 - ~@parfor_demo~ refers to the function name (parfor_demo.m).
 - The file must already exist in the specified remote directory.
 - ~CurrentFolder~ tells TRUBA where to execute the job.
 - ~AutoAddClientPath = false~ prevents MATLAB from trying to
   stage local files to the cluster.
 - ~Pool = 55~ results in 56 Slurm tasks (55 workers + 1 client).

 In this mode, no file transfer occurs. MATLAB simply instructs
 Slurm to execute the script directly from the shared TRUBA filesystem.


 #+begin_example
# Here is the output:
Submit arguments: --ntasks=56 --cpus-per-task=1 -D /arf/scratch/$USER --ntasks-per-core=1 -t 0-00:30 -N 1 -p orfoz
>> j.wait;
>> j.diary
--- Start Diary ---
parfor_demo: N=55, elapsed=1.85 s

--- End Diary ---
 #+end_example
 



** Running Interactively on TRUBA with ~parpool~

In this method, you start a parallel pool interactively and then run the
same ~parfor_demo~ function as usual. The pool workers are started on TRUBA
compute nodes through Slurm.

#+begin_src matlab
% Get a handle to the cluster profile
c = parcluster;

% Target the orfoz partition (56 cores per node policy)
c.AdditionalProperties.Partition = 'orfoz';
c.AdditionalProperties.NumNodes  = 1;
c.AdditionalProperties.WallTime  = '0-00:30';
#+end_src

*** Set the client hostname (recommended)

Before creating the pool, set the client hostname. This helps MATLAB workers
connect back to your client process reliably (especially in environments with
multiple network interfaces).

#+begin_src
sethostname
#+end_src

*** Create the cluster pool

For a half-node job on ~orfoz~, request a 56-task allocation and start a pool
with 56 workers.

#+begin_src matlab
pool = parpool(c, 56)
#+end_src

Show the screenshot here.

#+attr_html: :width 900px
#+caption: Creating a cluster-backed parallel pool on orfoz.
[[file:2_a.png]]

At this point, a Slurm job is submitted in the background and the pool becomes
available. After the pool is connected, you can run your code exactly as if you
were working in a regular MATLAB session.

*** Run the demo code

#+begin_src matlab
parfor_demo
#+end_src

Show the screenshot here.

#+attr_html: :width 900px
#+caption: Running ~parfor_demo~ using the active cluster pool.
[[file:2_b.png]]

*** Close the pool (release resources)

When you are finished, delete the pool to release the Slurm allocation and
compute resources.

#+begin_src matlab
delete(pool)   % equivalent to: pool.delete
#+end_src


 



** Submitting via a Slurm Batch Script (Recommended HPC Method)

This method runs MATLAB entirely inside a Slurm allocation.
Slurm allocates the CPU resources, and MATLAB creates a local
parallel pool that uses those allocated cores.

This is the most robust and recommended approach for production workloads.

*** MATLAB Script (run_parfor.m)

Create the following file in your working directory:

#+begin_src matlab
% run_parfor.m

% Read number of workers from environment variable
n = str2double(getenv("NUM_WORKERS"));

% Start a local parallel pool using allocated CPUs
parpool('local', n);

% Run the demo function
parfor_demo();

% Clean up the pool
delete(gcp('nocreate'));
#+end_src

Explanation:

- ~NUM_WORKERS~ is passed from Slurm.
- ~parpool('local', n)~ starts workers using the CPUs already allocated.
- No Slurm plugin or cluster profile is used.
- The pool is deleted at the end to release resources cleanly.

*** Slurm Script (submit_parfor.sh)

#+begin_src bash
#!/bin/bash
#SBATCH -J matlab-parfor
#SBATCH -A sbilmis
#SBATCH -p orfoz
#SBATCH -N 1
#SBATCH --ntasks=56
#SBATCH --cpus-per-task=1
#SBATCH -t 0-00:30
#SBATCH -o matlab-parfor_%j.out
#SBATCH -e matlab-parfor_%j.err

module load apps/matlab/r2025b

# Use one task for MATLAB client, remaining for workers
export NUM_WORKERS=$((SLURM_NTASKS - 1))

matlab -batch "run('run_parfor.m')"
#+end_src

Key points:

- ~--ntasks=56~ reserves 56 CPU cores on ~orfoz~.
- One core runs the main MATLAB process.
- 55 cores are used as parallel workers.
- ~NUM_WORKERS~ is computed automatically from ~SLURM_NTASKS~.
- If ~--ntasks~ changes, worker count adjusts automatically.

*** Submitting the Job

#+begin_src bash
sbatch submit_parfor.sh
#+end_src

Monitor job status:

#+begin_src bash
squeue -u sbilmis
#+end_src

*** Output File

After completion, inspect:

#+begin_src bash
cat matlab-parfor_<jobid>.out
#+end_src

Example output:

#+begin_example

	Sponsored License -- for use in support of a program or activity sponsored by MathWorks.
	Not for government, commercial or other non-sponsored organizational use.

Starting parallel pool (parpool) using the 'Processes' profile ...
Connected to parallel pool with 55 workers.
parfor_demo: N=55, elapsed=1.29 s
Parallel pool using the 'Processes' profile is shutting down.
#+end_example








** Running MATLAB via Open OnDemand (Interactive Desktop)

Open OnDemand allows users to launch a graphical desktop session
on TRUBA compute nodes through a web browser. This is equivalent
to running an interactive Slurm job.

In this mode, Slurm allocates the resources first, and MATLAB runs
inside that allocation.

---

*** Step 1: Launch Desktop Session

1. Log in to Open OnDemand.
2. Select *Interactive Apps â†’ Desktop*.
3. Choose the following parameters:

   - Desktop Environment: xfce
   - Partition: orfoz
   - Time limit: 1 hour
   - Account: sbilmis

4. Click *Launch*.

#+attr_html: :width 900px
#+caption: Open OnDemand Desktop launch configuration (orfoz partition).
[[file:images/ood_desktop_launch.png]]

---

*** Step 2: Open Terminal

Once the desktop loads:

1. Right-click on the desktop
2. Select *Open Terminal Here*

#+attr_html: :width 700px
#+caption: Opening a terminal inside the interactive desktop session.
[[file:images/ood_open_terminal.png]]

---

*** Step 3: Load MATLAB Module

In the terminal:

#+begin_src bash
module purge
module load apps/matlab/r2025b
matlab
#+end_src

#+attr_html: :width 900px
#+caption: Loading MATLAB module inside Open OnDemand desktop.
[[file:images/ood_terminal_module.png]]

---
*** Step 4: Submit Job Using batch()

Instead of running ~parpool~ interactively, we submit the job
using the TRUBA cluster profile. This allows the job to continue
running even after MATLAB is closed.

Inside the MATLAB command window:

#+begin_src matlab
c = parcluster;

c.AdditionalProperties.Partition = 'orfoz';
c.AdditionalProperties.NumNodes  = 1;
c.AdditionalProperties.WallTime  = '0-00:30';

j = batch(c, @parfor_demo, 1, {}, ...
    'Pool', 55, ...
    'CurrentFolder', '/arf/scratch/sbilmis', ...
    'AutoAddClientPath', false);

# or one line
j = batch(c, @parfor_demo, 1, {}, 'Pool', 55, 'CurrentFolder', '/arf/scratch/sbilmis', 'AutoAddClientPath', false);

    #+end_src


Explanation:

- ~Pool = 55~ results in 56 Slurm tasks (55 workers + 1 client).
- ~CurrentFolder~ specifies where the function exists on TRUBA.
- The job is now managed entirely by Slurm.
- You may safely close MATLAB after submission.

#+attr_html: :width 900px
#+caption: Submitting parfor_demo using batch() inside Open OnDemand.
[[file:ood_batch_submit.png]]

---

*** Step 5: Close MATLAB (Optional)

After submission, you may close MATLAB and even close
the Open OnDemand desktop session.

The job continues running on TRUBA.

---

*** Step 6: Monitor Job from Terminal

From any login node:

#+begin_src bash
squeue -u sbilmis
#+end_src

To check detailed job information:

#+begin_src bash
scontrol show job <jobid>
#+end_src


#+attr_html: :width 900px
#+caption: Monitoring submitted MATLAB job via squeue.
[[file:ood_squeue.png]]


---

*** Step 7: Retrieve Output Without MATLAB

Cluster profile jobs store results under:

#+begin_example
/arf/home/sbilmis/.matlab/generic_cluster_jobs/truba/
#+end_example

List jobs:

#+begin_src bash
ls /arf/home/sbilmis/.matlab/generic_cluster_jobs/truba/
#+end_src

Each job has a folder:

#+begin_example
Job1/
Job2/
Job3/
#+end_example

To read printed output (diary):

#+begin_src bash
cat /arf/home/sbilmis/.matlab/generic_cluster_jobs/truba/JobX/diary
#+end_src

Example output:

#+begin_example
parfor_demo: N=55, elapsed=1.83 s
#+end_example

#+begin_example
ðŸ“˜ sbilmis@arf-ui1  ~$ grep "parfor_demo:" ~/.matlab/generic_cluster_jobs/truba/Job3/Task*.diary.txt
/arf/home/sbilmis/.matlab/generic_cluster_jobs/truba/Job3/Task1.diary.txt:parfor_demo: N=55, elapsed=1.89 s
#+end_example


---

*** Step 8: (Optional) Retrieve Output Later in MATLAB

If you reopen MATLAB later, you can recover the job object:

#+begin_src matlab
c = parcluster;
jobs = c.Jobs;
j = jobs(end);
t = fetchOutputs(j);
#+end_src

This allows retrieval of returned variables without rerunning the job.


* Further Reading

To learn more about MATLAB Parallel Computing Toolbox,
the following official MathWorks resources may be helpful:

- [[https://www.mathworks.com/products/parallel-computing/index.html][Parallel Computing Overview]]
- [[https://www.mathworks.com/help/parallel-computing/][Parallel Computing Documentation]]
- [[https://www.mathworks.com/help/parallel-computing/examples.html][Parallel Computing Coding Examples]]
- [[https://www.mathworks.com/products/parallel-computing/tutorials.html][Parallel Computing Tutorials]]
- [[https://www.mathworks.com/products/parallel-computing/videos.html][Parallel Computing Videos]]
- [[https://www.mathworks.com/products/parallel-computing/webinars.html][Parallel Computing Webinars]]

These resources cover general usage of parallel computing within MATLAB and are not specific to TRUBA cluster configuration.



* Quick Reference (Cheatsheet)

You tell MATLAB how to "book" your resources using the =AdditionalProperties= command. Here is the minimal set you should always define:

    #+begin_src matlab
# set partition name
  c.AdditionalProperties.Partition = 'debug';
# set time limit
  c.AdditionalProperties.WallTime = '0-00:10'; % Format is days-hours:minutes
# set node count
  c.AdditionalProperties.NumNodes = 1; % For most beginners, starting with 1 node is best.
#+end_src

** What Happens After You Submit a Job?

After submission, the job enters the Slurm queue.
The start time depends on:

- Current cluster load
- Partition priority
- Requested resources (nodes, GPUs, walltime)

The job may remain in =queued= state for some time before running.

*** You Can Close MATLAB

Once the job is submitted:

- You may close MATLAB.
- You may shut down your desktop session.
- The job continues to run on TRUBA.

To check the job later:

#+begin_src matlab
c = parcluster;
c.Jobs
#+end_src

Or from the login node:

#+begin_src bash
squeue -u $USER
#+end_src



















































** Example: Running on the =orfoz= Partition (56-core half-node)

On =orfoz=, TRUBA enforces a per-node CPU policy: jobs must request
56 or 112 CPU cores per node.

IMPORTANT:
For MATLAB pool jobs, the Slurm plugin requests:

- =Total Slurm tasks = Pool + 1=

(The extra 1 task is used for job orchestration.)

Therefore, to request *56 cores per node* on =orfoz=, you must set:

- =Pool = 55=  â†’ Slurm submits =--ntasks=56=

*** Correct example (half node on =orfoz=)

#+begin_src matlab
c = parcluster;
c.AdditionalProperties.Partition = 'orfoz';
c.AdditionalProperties.NumNodes  = 1;
c.AdditionalProperties.WallTime  = '0-00:30';

j = batch(c, @pwd, 1, {}, ...
    'Pool', 55, ...
    'CurrentFolder', '.', ...
    'AutoAddClientPath', false);
#+end_src



IMPORTANT:
On partitions with per-node core policies (e.g., =orfoz=), always set:

- =c.AdditionalProperties.NumNodes=

If =NumNodes= is left unset, Slurm may place the job in a way that violates per-node policies.


For a half-node job on =orfoz=, use:

- =NumNodes = 1=
- =Pool = 55=  (because MATLAB requests =Pool+1= tasks â†’ 56 total)


*** Common mistake (will be rejected)

If you use =Pool=56=, MATLAB will submit =--ntasks=57=, and Slurm rejects it:

#+begin_example
sbatch: error: Orfoz kuyruguna gonderilen islerde node basina 56/112 cekirdek talep ediniz.
#+end_example


*** Slurm Command Generated by MATLAB

When you submit the job, MATLAB prints the exact Slurm
arguments used for submission:

#+begin_example
Submit arguments:
--ntasks=55 --cpus-per-task=1 -D /arf/scratch/$USER \
--ntasks-per-core=1 -t 0-3:00 -C orfoz -N 1 -p orfoz
#+end_example

This shows how MATLAB parameters map to Slurm options:

- =Pool= â†’ total tasks (--ntasks)
- =NumNodes= â†’ -N
- =Partition= â†’ -p
- =Constraint= â†’ -C
- =WallTime= â†’ -t


** Example: Submitting a GPU Job (=barbun-cuda=)
   
This section shows a minimal *GPU* test job submission from MATLAB to TRUBA.

*** Key policy on =barbun-cuda=

TRUBA enforces CPUâ†”GPU request rules on GPU partitions.

On =barbun-cuda=:

- A GPU request is mandatory.
- CPU cores per node must be requested in multiples of 20.
- Policy requires *1 GPU per 20 CPU cores per node*.

Since MATLAB pool jobs submit an extra orchestration task, the Slurm plugin typically requests:

- =Total Slurm tasks = Pool + 1=

This matters when matching the required CPU counts.

*** Hardware note

The =barbun-cuda= nodes provide *2 GPUs per node*.

A common valid request for one node is therefore:

- 2 GPUs per node
- 40 CPU cores per node

*** Minimal valid GPU test job (2 GPUs, 40 CPUs on 1 node)

Configure the cluster profile:

#+begin_src matlab
c = parcluster;

% Target partition
c.AdditionalProperties.Partition = 'barbun-cuda';

% Request 1 node for the test
c.AdditionalProperties.NumNodes = 1;

% Set a short walltime for testing
c.AdditionalProperties.WallTime = '0-00:30';

% Request 2 GPUs on the node
c.AdditionalProperties.GPUsPerNode = 2;
#+end_src

Submit a minimal job.

To satisfy the 40-CPU requirement on one node, set:

- =Pool = 39=  â†’ Slurm submits =--ntasks=40=

#+begin_src matlab
j = batch(c, @gpuDeviceCount, 1, {}, ...
    'Pool', 39, ...
    'CurrentFolder', '.', ...
    'AutoAddClientPath', false);
#+end_src

Fetch and inspect results:

#+begin_src matlab
wait(j);
out = fetchOutputs(j);
out{1}
#+end_src

Expected output is the number of GPUs visible to the worker (typically 2).

*** Slurm command generated by MATLAB

When submitting, MATLAB prints the Slurm arguments used by the plugin.
A successful submission should include:

- =-p barbun-cuda=
- =-N 1=
- =--ntasks=40=
- a GPU request (gres / TRES)

#+begin_example
Submit arguments: --ntasks=40 --cpus-per-task=1 -D /arf/scratch/$USER --ntasks-per-core=1 -t 0-00:30 --gres=gpu:2 -N 1 -p barbun-cuda
#+end_example

Use this output to verify that the request matches TRUBA policy
(e.g., CPUs and GPUs requested per node).

*** Common mistakes (and what happens)

**** Missing GPU request

If you submit to =barbun-cuda= without requesting a GPU, Slurm will reject the job.

Typical message:

#+begin_example
barbn-cuda kuyruguna sadece GPU talebi olan isler gonderilebilir.
#+end_example

**** CPU/GPU mismatch

If you request 2 GPUs but do not request 40 CPUs per node (multiples of 20 per GPU),
Slurm will reject the job.

Examples that are likely to fail:

- Too few CPUs for 2 GPUs (e.g., 20 CPUs with 2 GPUs)
- CPU count not a multiple of 20

*** Tips

- Start GPU tests with short walltime and 1 node.
- Keep your working directory under =/arf/scratch=.
- If you see a policy-related rejection, first check:
  - =Partition=
  - =NumNodes=
  - =GPUsPerNode=
  - =Pool= (remember =Pool + 1= tasks)












** Example: Parallel Batch Jobs: Scaling Up Your Code              :noexport:
 While "Serial" jobs run on one CPU, a **Parallel Batch Job** allows you to use multiple CPUs (Workers) at the same time. This is the best way to speed up heavy simulations or data processing.

*** The Example Script
 To follow this example, save the following code as a file named =parallel_example.m= in your current folder. This script simulates a heavy task by "pausing" for 2 seconds in each loop.

 
 #+begin_src matlab
function [sim_t, A] = parallel_example(iter)
    if nargin==0
        iter = 8; % Default to 8 loops if no input is given
    end
    
    disp('Starting Parallel Simulation...')
    A = nan(iter,1);
    t0 = tic;
    
    % The "parfor" loop runs these iterations in parallel
    parfor idx = 1:iter
        A(idx) = idx;
        pause(2) % Simulates a 2-second calculation
    end
    
    sim_t = toc(t0);
    disp('Simulation Completed.')
    save RESULTS A
end
 #+end_src

*** Submitting a Parallel Job
 To run this script on the cluster using 4 workers, use the =Pool= argument.

 #+begin_src matlab
% 1. Get a handle to the cluster
c = parcluster;

c.AdditionalProperties.Partition = 'debug'
c.AdditionalProperties.NumNodes = 1
c.AdditionaProperties.WallTime = '0-00:10'

% 2. Submit the job with a Pool of 4 workers
% We are asking for 16 simulations ({16}) to be split among them
job = c.batch(@parallel_example, 1, {16}, 'CurrentFolder', '.', 'Pool', 4);
 #+end_src


*** Important: The "N+1" Rule
 When you ask for a Pool of **4**, MATLAB actually requests **5** CPU cores from TRUBA.
 - **1 Manager Core:** Stays active to coordinate the work.
 - **4 Worker Cores:** Do the actual math.

 #+BEGIN_TIP
 *Rookie Tip:* If your job is rejected by a TRUBA partition like ~orfoz~ (which requires 56 cores), remember to set your Pool to **55**. The Manager core will make it exactly 56.
 #+END_TIP



*** Recovering Results Using a Job ID
 One of the best features of Batch jobs is that you can close your computer and come back later. To do this, you need the **Job ID**.

 - **Step 1: Get the ID after submission**
   #+begin_src matlab
  id = job.ID  % For example, it might return: 4
   #+end_src

 - **Step 2: Recover the job later**
   If you have closed and reopened MATLAB, you can "find" your job again using that ID:
   #+begin_src matlab
  c = parcluster;
  job = c.findJob('ID', 4); % Use the ID you recorded earlier
   #+end_src

 - **Step 3: Fetch the results**
   Once the state is 'finished', retrieve your data:
   #+begin_src matlab
  results = fetchOutputs(job);
  time_taken = results{1}
   #+end_src



*** Understanding Efficiency and Scaling
 Is more always better? Not necessarily.
 - **Scaling Up:** If you run 16 simulations with 4 workers, it might take 8.8 seconds. If you use 8 workers, it might drop to 4.7 seconds.
 - **The Limit:** Every parallel job has "Overhead" (the time it takes for the Boss to talk to the Workers). If your task is very small, adding more workers might actually make it **slower** because the communication takes longer than the math.

 #+BEGIN_NOTE
 **Recommendation:** Start with a small pool (4 or 8) to test your code. Gradually increase the number of workers to find the "Sweet Spot" where your code runs fastest without wasting cluster resources.
 #+END_NOTE

*** Alternative: The Graphical Job Monitor
 If you prefer not to use code to find your jobs, you can use the built-in GUI:
 - Go to the **Parallel** menu in the Home tab.
 - Select **Monitor Jobs**.
 - Here you can see all your IDs, check their status, and right-click to fetch results or view errors.









* MY TASKS                                                         :noexport:
- [ ] In OpenOnDemand (172.16.6.20) either fix matlab or remove from the options. 
- [X] configCluster (Why there are 3 options) (Old configuration)
Ensure you pick a cluster you have access to.
	[1] truba
	[2] truba-arf
	[3] truba-cuda

- [ ] You can constrain the nodes in the partition, for instance send jobs to debug partition with orfoz (c.AdditionalProperties.Constraint = 'orfoz'). Note that if you choose debug partition and constrain the node then there is no obligation to choose 56/112, however if you choose directly orfoz partition then you need to. 

- [ ] Adding a special matlab cluster to easily create pools (no need restrictions like 56xn etc)
























* License Requirement for Remote Job Submission                    :noexport:

IMPORTANT:

TRUBA operates under a sponsored academic MATLAB license.

To submit jobs from MATLAB installed on your personal computer
to the TRUBA cluster, the MATLAB installation on your local machine
must also be covered by a valid academic license.

Remote job submission requires:

- A valid academic MATLAB license on the local machine.
- Access to the TRUBA-sponsored MATLAB license on the cluster.

Users without an academic MATLAB license on their local machine
cannot submit jobs remotely via the cluster profile method.

In such cases, users should connect to TRUBA via Open OnDemand
and run MATLAB directly on the cluster.

































